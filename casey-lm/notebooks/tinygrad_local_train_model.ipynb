{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow imports from the project directory\n",
    "import sys\n",
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crhird/Documents/Projects/casey-lm/casey-lm-venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/crhird/Documents/Projects/casey-lm/casey-lm-venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1821/1821 [00:00<00:00, 14443.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from data.glue_dataloader import gen_dataloaders\n",
    "batch_size = 32\n",
    "max_length = 128\n",
    "\n",
    "train_dataloader, val_dataloader, vocab_size = gen_dataloaders(batch_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2105 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 127]) torch.Size([32, 127])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "stop here",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m             accuracy \u001b[38;5;241m=\u001b[39m total_correct \u001b[38;5;241m/\u001b[39m num_val_examples\n\u001b[1;32m     77\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m \u001b[43mdo_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 48\u001b[0m, in \u001b[0;36mdo_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mshape, labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop here\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m loss \u001b[38;5;241m=\u001b[39m step(model, inputs, labels, optimizer)\n\u001b[1;32m     51\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mException\u001b[0m: stop here"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from model.tinygrad_impl.language_model import TinygradLanguageModel, TinygradLanguageModelConfig\n",
    "from model.tinygrad_impl.transformer import TransformerConfig, TransformerDecoderLayerConfig\n",
    "from model.tinygrad_impl.mlp import MLPConfig\n",
    "from tinygrad import Tensor\n",
    "import tinygrad\n",
    "\n",
    "\n",
    "def step(model, X, Y, optim):\n",
    "    optim.zero_grad()\n",
    "    # loss = criterion(output.view(-1, tokenizer.vocab_size), labels.flatten()\n",
    "    # TODO: debug tinygrad model\n",
    "    loss = model(X).cross_entropy(Y).backward()\n",
    "    optim.step()\n",
    "    return loss\n",
    "\n",
    "def do_train():\n",
    "    model = TinygradLanguageModel(TinygradLanguageModelConfig(\n",
    "        vocab_size=vocab_size,\n",
    "        context_length=max_length,\n",
    "        embedding_dim=8,\n",
    "        transformer_config=TransformerConfig(\n",
    "        num_decoder_layers=3,\n",
    "        decoder_layer_config=TransformerDecoderLayerConfig(\n",
    "            d_model=8,\n",
    "            n_head=2,\n",
    "            dim_feedforward=32,\n",
    "            dropout_p=0.1,\n",
    "            )\n",
    "        ),\n",
    "        mlp_config=MLPConfig(\n",
    "            d_model=8,\n",
    "            d_hidden=16,\n",
    "            num_layers=3,\n",
    "            dropout_p=0.1,\n",
    "        )\n",
    "    ))\n",
    "    optimizer = tinygrad.nn.optim.Adam(tinygrad.nn.state.get_parameters(model))\n",
    "\n",
    "    for epoch in range(2):\n",
    "        Tensor.training = True  # makes dropout work\n",
    "        train_loss = 0.0\n",
    "        num_train_examples = 0\n",
    "        num_batches = 0\n",
    "        for batch in tqdm.tqdm(train_dataloader):\n",
    "            inputs = batch[:, :-1]\n",
    "            labels = batch[:, 1:]\n",
    "\n",
    "            loss = step(model, inputs, labels, optimizer)\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            num_train_examples += labels.numel()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            num_batches += 1\n",
    "            if num_batches > 5:\n",
    "                break\n",
    "\n",
    "        train_loss /= num_train_examples\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {train_loss:.4f}')\n",
    "\n",
    "        Tensor.training = False\n",
    "        total_correct = 0\n",
    "        num_val_examples = 0\n",
    "        num_batches = 0\n",
    "        for batch in val_dataloader:\n",
    "            inputs = batch[:, :-1]\n",
    "            labels = batch[:, 1:]\n",
    "            output = model(inputs)\n",
    "            _, predicted = output.max(dim=2)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            num_val_examples += labels.numel()\n",
    "            num_batches += 1\n",
    "            if num_batches > 5:\n",
    "                break\n",
    "            accuracy = total_correct / num_val_examples\n",
    "            print(f'Epoch {epoch+1}, Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "do_train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casey-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
